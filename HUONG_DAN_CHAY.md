# H∆∞·ªõng D·∫´n Ch·∫°y Training v√† Inference

## üìã T·ªïng Quan

Code n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø cho **Google Cloud TPU v3-8** v√† **TensorFlow 1.15**. ƒê·ªÉ ch·∫°y tr√™n Colab/Kaggle, b·∫°n c·∫ßn:
- TensorFlow 1.15 (ho·∫∑c t∆∞∆°ng th√≠ch)
- TPU runtime (Colab) ho·∫∑c GPU (Kaggle)
- Google Cloud Storage bucket (ho·∫∑c s·ª≠a code ƒë·ªÉ d√πng local storage)

---

## üéØ Option 1: Inference v·ªõi Model Pretrained (KHUY·∫æN NGH·ªä)

### B∆∞·ªõc 1: Download Pretrained Model

Models v√† samples c√≥ s·∫µn t·∫°i: https://www.dropbox.com/sh/pm6tn31da21yrx4/AABWKZnBzIROmDjGxpB6vn6Ja

**C√°ch download:**
```bash
# Tr√™n Colab/Kaggle
!wget -O model.zip "https://www.dropbox.com/sh/pm6tn31da21yrx4/AABWKZnBzIROmDjGxpB6vn6Ja?dl=1"
!unzip model.zip
```

### B∆∞·ªõc 2: Ch·∫°y Inference

#### C√°ch 1: D√πng `simple_eval` (ƒë∆°n gi·∫£n nh·∫•t)

```bash
# V√≠ d·ª• cho CIFAR-10
python3 scripts/run_cifar.py simple_eval \
  --model_dir /path/to/model/checkpoint \
  --tpu_name your-tpu-name \
  --bucket_name_prefix your-bucket-prefix \
  --mode progressive_samples \
  --load_ckpt model.ckpt-1000000 \
  --total_bs 64
```

**C√°c mode c√≥ s·∫µn:**
- `progressive_samples`: T·∫°o samples v√† l∆∞u progressive predictions
- `bpd_train`: T√≠nh bits-per-dimension tr√™n training set
- `bpd_eval`: T√≠nh bits-per-dimension tr√™n eval set

#### C√°ch 2: D√πng `evaluation` (ƒë·∫ßy ƒë·ªß h∆°n)

```bash
# T·∫°o samples v√† t√≠nh metrics
python3 scripts/run_cifar.py evaluation \
  --model_dir /path/to/model/checkpoint \
  --tpu_name your-tpu-name \
  --bucket_name_prefix your-bucket-prefix \
  --once True \
  --dump_samples_only True \
  --total_bs 64
```

**Tham s·ªë quan tr·ªçng:**
- `--model_dir`: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a checkpoint
- `--tpu_name`: T√™n TPU (ho·∫∑c `None` n·∫øu ch·∫°y tr√™n CPU/GPU)
- `--bucket_name_prefix`: Prefix c·ªßa GCS bucket
- `--load_ckpt`: T√™n checkpoint (v√≠ d·ª•: `model.ckpt-1000000`)
- `--once`: Ch·ªâ ch·∫°y 1 l·∫ßn (kh√¥ng loop)
- `--dump_samples_only`: Ch·ªâ t·∫°o samples, kh√¥ng t√≠nh metrics

---

## üöÄ Option 2: Training t·ª´ ƒë·∫ßu

### B∆∞·ªõc 1: Setup Environment

```bash
# C√†i ƒë·∫∑t dependencies
pip3 install fire scipy pillow
pip3 install tensorflow-probability==0.8
pip3 install tensorflow-gan==0.0.0.dev0
pip3 install tensorflow-datasets==2.1.0
```

**L∆∞u √Ω:** TensorFlow 1.15 c√≥ th·ªÉ kh√¥ng t∆∞∆°ng th√≠ch v·ªõi Python m·ªõi. C√¢n nh·∫Øc d√πng Docker ho·∫∑c virtualenv.

### B∆∞·ªõc 2: Setup GCS Bucket

```bash
# T·∫°o bucket tr√™n Google Cloud
gsutil mb gs://your-bucket-prefix-us-central1

# Upload dataset (n·∫øu c·∫ßn)
gsutil cp -r /local/dataset gs://your-bucket-prefix-us-central1/tensorflow_datasets
```

### B∆∞·ªõc 3: Ch·∫°y Training

#### CIFAR-10:
```bash
python3 scripts/run_cifar.py train \
  --exp_name my_experiment \
  --tpu_name your-tpu-name \
  --bucket_name_prefix your-bucket-prefix \
  --model_name unet2d16b2 \
  --dataset cifar10 \
  --total_bs 128 \
  --lr 2e-4 \
  --num_diffusion_timesteps 1000 \
  --beta_start 0.0001 \
  --beta_end 0.02
```

#### CelebA-HQ:
```bash
python3 scripts/run_celebahq.py train \
  --exp_name celebahq_experiment \
  --tpu_name your-tpu-name \
  --bucket_name_prefix your-bucket-prefix \
  --total_bs 64 \
  --lr 0.00002
```

#### LSUN:
```bash
python3 scripts/run_lsun.py train \
  --exp_name lsun_church \
  --tpu_name your-tpu-name \
  --bucket_name_prefix your-bucket-prefix \
  --tfr_file 'tensorflow_datasets/lsun/church/church-r08.tfrecords' \
  --total_bs 64
```

### B∆∞·ªõc 4: Monitor Training

Checkpoints ƒë∆∞·ª£c l∆∞u t·∫°i: `gs://your-bucket-prefix-us-central1/logs/your_experiment_name/`

Xem logs:
```bash
tensorboard --logdir=gs://your-bucket-prefix-us-central1/logs
```

---

## üíª Ch·∫°y tr√™n Colab

### Setup Colab Notebook:

```python
# Cell 1: Install dependencies
!pip3 install fire scipy pillow tensorflow-probability==0.8 tensorflow-gan==0.0.0.dev0 tensorflow-datasets==2.1.0

# Cell 2: Clone repo (ho·∫∑c upload code)
!git clone https://github.com/hojonathanho/diffusion.git
%cd diffusion

# Cell 3: Authenticate GCP (n·∫øu d√πng GCS)
from google.colab import auth
auth.authenticate_user()

# Cell 4: Setup TPU
import tensorflow as tf
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_name = tpu.get_master()

# Cell 5: Download pretrained model (n·∫øu c√≥)
!wget -O model.zip "https://www.dropbox.com/sh/pm6tn31da21yrx4/AABWKZnBzIROmDjGxpB6vn6Ja?dl=1"
!unzip model.zip

# Cell 6: Ch·∫°y inference
!python3 scripts/run_cifar.py simple_eval \
  --model_dir ./checkpoints/cifar10 \
  --tpu_name $tpu_name \
  --bucket_name_prefix your-bucket \
  --mode progressive_samples \
  --load_ckpt model.ckpt-1000000
```

**L∆∞u √Ω:** Colab c√≥ th·ªÉ kh√¥ng h·ªó tr·ª£ TensorFlow 1.15. C√¢n nh·∫Øc upgrade code l√™n TF 2.x ho·∫∑c d√πng runtime c≈©.

---

## üéÆ Ch·∫°y tr√™n Kaggle

Kaggle **KH√îNG c√≥ TPU**, n√™n b·∫°n c·∫ßn:

1. **S·ª≠a code ƒë·ªÉ ch·∫°y tr√™n GPU/CPU** (kh√¥ng d√πng TPU)
2. **Thay GCS b·∫±ng local storage** (`/kaggle/working/`)
3. **C√†i TensorFlow 1.15** (c√≥ th·ªÉ kh√≥ khƒÉn)

**Khuy·∫øn ngh·ªã:** Ch·ªâ d√πng Kaggle ƒë·ªÉ inference v·ªõi model ƒë√£ train s·∫µn, ho·∫∑c port code sang PyTorch.

---

## üìù C√°c Tham S·ªë Quan Tr·ªçng

### Training Parameters:
- `--total_bs`: Batch size t·ªïng (chia cho s·ªë TPU cores)
- `--lr`: Learning rate (th∆∞·ªùng 2e-4 cho CIFAR, 2e-5 cho LSUN)
- `--num_diffusion_timesteps`: S·ªë timesteps (th∆∞·ªùng 1000)
- `--beta_start`, `--beta_end`: Beta schedule range
- `--model_mean_type`: `'eps'` (predict noise) ho·∫∑c `'xstart'` (predict x_0)
- `--loss_type`: `'mse'` ho·∫∑c `'kl'`

### Inference Parameters:
- `--mode`: `progressive_samples`, `bpd_train`, `bpd_eval`
- `--load_ckpt`: T√™n checkpoint file (kh√¥ng c√≥ extension)
- `--total_bs`: Batch size cho sampling
- `--dump_samples_only`: Ch·ªâ t·∫°o samples, b·ªè qua metrics

---

## üîß Troubleshooting

### L·ªói: "TPU not found"
- Ki·ªÉm tra TPU ƒë√£ ƒë∆∞·ª£c t·∫°o v√† k·∫øt n·ªëi
- Tr√™n Colab: Runtime ‚Üí Change runtime type ‚Üí TPU

### L·ªói: "Bucket not found"
- Ki·ªÉm tra GCS bucket ƒë√£ ƒë∆∞·ª£c t·∫°o
- Ki·ªÉm tra quy·ªÅn truy c·∫≠p (authentication)

### L·ªói: "TensorFlow version mismatch"
- Code c·∫ßn TF 1.15, nh∆∞ng m√¥i tr∆∞·ªùng c√≥ TF 2.x
- Gi·∫£i ph√°p: D√πng Docker ho·∫∑c virtualenv v·ªõi TF 1.15

### L·ªói: "Checkpoint not found"
- Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n `--model_dir` ƒë√∫ng
- Ki·ªÉm tra file checkpoint t·ªìn t·∫°i: `model.ckpt-*.index`, `model.ckpt-*.data-*`

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

- Paper: https://arxiv.org/abs/2006.11239
- Website: https://hojonathanho.github.io/diffusion
- Pretrained Models: https://www.dropbox.com/sh/pm6tn31da21yrx4/AABWKZnBzIROmDjGxpB6vn6Ja

---

## ‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng

1. **Code n√†y r·∫•t c≈©** (TF 1.15, 2020), c√≥ th·ªÉ kh√¥ng ch·∫°y ƒë∆∞·ª£c tr√™n m√¥i tr∆∞·ªùng hi·ªán ƒë·∫°i
2. **C·∫ßn TPU ho·∫∑c GPU m·∫°nh** ƒë·ªÉ training
3. **GCS bucket l√† b·∫Øt bu·ªôc** tr·ª´ khi b·∫°n s·ª≠a code ƒë·ªÉ d√πng local storage
4. **Khuy·∫øn ngh·ªã:** N·∫øu ch·ªâ c·∫ßn inference, download pretrained model v√† ch·∫°y `simple_eval`


